<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Improving Generalization of Meta-learning with Inverted Regularization at Inner-level</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Despite the broad interest in meta-learning, the generalization problem remains one of the significant challenges in this field. Existing works focus on meta-generalization to unseen tasks at the meta-level by regularizing the metaloss, while ignoring that adapted models may not generalize to the task domains at the adaptation level. In this paper, we propose a new regularization mechanism for metalearning – Minimax-Meta Regularization, which employs inverted regularization at the inner loop and ordinary regularization at the outer loop during training. In particular, the inner inverted regularization makes the adapted model more difficult to generalize to task domains; thus, optimizing the outer-loop loss forces the meta-model to learn metaknowledge with better generalization. Theoretically, we prove that inverted regularization improves the meta-testing performance by reducing generalization errors. We conduct extensive experiments on the representative scenarios, and the results show that our method consistently improves the performance of meta-learning algorithms.">
<meta name="keywords" content="Meta-leanring; Generalization; Regularization; Generalization Bound; Few-shot Learning; Deep Learning">
<link rel="author" href="https://lianzhewang.github.io/">

<!-- Fonts and stuff -->
<link href="./meta-invert-reg/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./meta-invert-reg/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./meta-invert-reg/iconize.css">
<script async="" src="./meta-invert-reg/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>Improving Generalization of Meta-learning with Inverted Regularization at Inner-level</h1>

	<div class="authors">
	  <a href="https://lianzhewang.github.io/">Lianzhe Wang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://arnoldshijizhou.github.io/">Shiji Zhou</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://www.shanghangzhang.com">Shanghang Zhang</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://scholar.google.com/citations?user=KrR9i8YAAAAJ&hl=en">Xu Chu</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://hchang95.github.io/">Heng Chang</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://ieeexplore.ieee.org/author/37065056700">Wenwu Zhu</a><sup>1</sup>
	</div>

	<div class="affiliations">
	  1. <a href="https://www.tsinghua.edu.cn/en/">Tsinghua University</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  2. <a href="https://www.berkeley.edu/">UC Berkeley</a>
	</div>

	<div class="venue">The IEEE/CVF Conference on Computer Vision and Pattern Recognition (<a href="https://cvpr2023.thecvf.com/" target="_blank">CVPR</a>) 2023 </div>

	<div class="venue"> <font color="red" size="2"></font> </div>
      </div>
      
      <center><img src="./meta-invert-reg/Intro23.png" border="0" width="85%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
	Despite the broad interest in meta-learning, the generalization problem remains one of the significant challenges in this field. Existing works focus on meta-generalization to unseen tasks at the meta-level by regularizing the metaloss, while ignoring that adapted models may not generalize to the task domains at the adaptation level. In this paper, we propose a new regularization mechanism for metalearning – Minimax-Meta Regularization, which employs inverted regularization at the inner loop and ordinary regularization at the outer loop during training. In particular, the inner inverted regularization makes the adapted model more difficult to generalize to task domains; thus, optimizing the outer-loop loss forces the meta-model to learn metaknowledge with better generalization. Theoretically, we prove that inverted regularization improves the meta-testing performance by reducing generalization errors. We conduct extensive experiments on the representative scenarios, and the results show that our method consistently improves the performance of meta-learning algorithms.
	</p>
      </div>
      
<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://lianzhewang.github.io/papers/CVPR2023-Meta-Invert-Reg.pdf" target="_blank" class="imageLink"><img src="./meta-invert-reg/paper23.png"></a><br>
		  <a href="https://lianzhewang.github.io/papers/CVPR2023-Meta-Invert-Reg.pdf" target="_blank">Paper</a>
		</div>
	      </li>

	      <li class="grid">
	      <div class="griditem">
		<a href="https://lianzhewang.github.io/slides/CVPR2023-Meta-Invert-Reg-Slides-Lianzhe-Wang.pdf" target="_blank" class="imageLink"><img src="./meta-invert-reg/slides23.png"></a><br>
		  <a href="https://lianzhewang.github.io/slides/CVPR2023-Meta-Invert-Reg-Slides-Lianzhe-Wang.pdf" target="_blank">Slides</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="Presentation">
	<h2>Presentation</h2>
	<br>
	<center>
	  <iframe width="560" height="315" src="https://www.youtube.com/embed/DbLGnKn3Iv4?si=prfHTGGnyfBuiYJH" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>

<br>
	    
<!-- <div class="section presentation">
	<h2>Presentation</h2>
	<center>
	  <ul>
            <li class="grid">
	      <div class="griditem">
		<a href="https://www.youtube.com/watch?v=BQZ5xKd5kis&t=1361" target="_blank" class="imageLink"><img src="./meta-invert-reg/video.png"></a><br>
		  <a href="https://www.youtube.com/watch?v=BQZ5xKd5kis&t=1361" target="_blank">Video Recording</a>
		</div>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="../papers/meta-invert-reg_slides.pdf" target="_blank" class="imageLink"><img src="./meta-invert-reg/slides.jpg"></a><br>
		  <a href="../papers/meta-invert-reg_slides.pdf" target="_blank">Slides</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>

<br> -->

<!-- <div class="section data">
	<h2>EVIS Dataset</h2>
	<br>
	<center>
      	<a href="https://github.com/LianzheWang/EVIS" target="_blank" class="imageLink"><img src="./meta-invert-reg/dataset.png" border="2" width="80%"></a><br>
      	<a href="https://github.com/LianzheWang/EVIS" target="_blank">Evolving-Image-Search (EVIS) dataset</a>
    </center>
    </div>

<br>	    
	    
<div class="section code">
	<h2>Code and Models</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/LianzheWang/Active-Gradual-Domain-Adaptation-Dataset-and-Approach" target="_blank" class="imageLink"><img src="./meta-invert-reg/code.png"></a><br>
		  <a href="https://github.com/LianzheWang/Active-Gradual-Domain-Adaptation-Dataset-and-Approach" target="_blank">Code and Models</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br> -->



<!-- <div class="section data">
	<h2>Blog</h2>
	<br>
	<center>
      	<a href="https://bair.berkeley.edu/blog/2019/05/13/oltr/" target="_blank" class="imageLink"><img src="./meta-invert-reg/blog.png" border="2" width="30%"></a><br>
      	<a href="https://bair.berkeley.edu/blog/2019/05/13/oltr/" target="_blank">Berkeley AI Research Blog</a>
    </center>
    </div>

<br> -->

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>
@inproceedings{wang2023improving,
  title={Improving Generalization of Meta-Learning With Inverted Regularization at Inner-Level},
  author={Wang, Lianzhe and Zhou, Shiji and Zhang, Shanghang and Chu, Xu and Chang, Heng and Zhu, Wenwu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={7826--7835},
  year={2023}
}
	  </pre>
	  </div>
      </div>

</body></html>
