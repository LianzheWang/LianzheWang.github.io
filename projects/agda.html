<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Active Gradual Domain Adaptation: Dataset and Approach</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Adapting deep neural networks to changing environments is critical in practical utility, especially for online web applications, where the model has access to some labeled data from the source domain, unlabeled data, and limited labels from a gradually changing target domain. In this paper, we deal with such problem via active gradual domain adaptation, where the learner continually and actively selects the most informative labels from the target to enhance the label efficiency and utilizes both labeled and unlabeled samples to improve the model adaptation under gradual domain drift. We propose the active gradual self-training (AGST) algorithm with the novel designs of active pseudolabeling and gradual semi-supervised domain adaptation. Specifically, AGST pseudolabels the samples with high confidence, and selects the most informative labels from the unconfident samples by both uncertainty and diversity, then gradually self-trains itself by confident pseudolabels, active queried informative labels, and data features. In our experiment, we create a new dataset -- Evolving-Image-Search (EVIS) collected from the web search engine with gradual domain drift. The experiment results on synthetic dataset, real-world dataset and EVIS dataset show that AGST achieves up to 62\% accuracy improvement against unsupervised gradual self-training with only 5\% additional labels, and 19\% than directly applying CLUE, which demonstrated the effectiveness of the designs of active pseudolabel and gradual semi-supervised domain adaptation.">
<meta name="keywords" content="Gradual Domain Drift; Gradual Domain Adaptation; Active Domain Adaptation; Web Noise Data; Uncertainty; Deep Learning">
<link rel="author" href="https://lianzhewang.github.io/">

<!-- Fonts and stuff -->
<link href="./agda/css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./agda/project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./agda/iconize.css">
<script async="" src="./agda/prettify.js"></script>


</head>

<body>
  <div id="content">
    <div id="content-inner">
      
      <div class="section head">
	<h1>Large-Scale Long-Tailed Recognition in an Open World</h1>

	<div class="authors">
	  <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>1,2</sup>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://github.com/zhmiao">Zhongqi Miao</a><sup>2</sup>*&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://xiaohangzhan.github.io/">Xiaohang Zhan</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://pwang.pw/">Jiayun Wang</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="http://boqinggong.info/">Boqing Gong</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  <a href="https://www1.icsi.berkeley.edu/~stellayu/">Stella X. Yu</a><sup>2</sup>
	</div>

	<div class="affiliations">
	  1. <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
	  2. <a href="https://www.berkeley.edu/">UC Berkeley / ICSI</a>
	</div>

	<div class="venue">IEEE Conference on Computer Vision and Pattern Recognition (<a href="http://cvpr2019.thecvf.com/" target="_blank">CVPR</a>) 2019, <font color="#e86e14">Oral Presentation</font> </div>

	<div class="venue"> <font color="red" size="2">(HKSTP Best Paper Award)</font> </div>
      </div>
      
      <center><img src="./agda/intro.png" border="0" width="90%"></center>

<div class="section abstract">
	<h2>Abstract</h2>
	<br>
	<p>
Real world data often have a long-tailed and open-ended distribution. A practical recognition system must classify among majority and minority classes, generalize from a few known instances, and acknowledge novelty upon a never seen instance. We define Open Long-Tailed Recognition (OLTR) as learning from such naturally distributed data and optimizing the classification accuracy over a balanced test set which include head, tail, and open classes. OLTR must handle imbalanced classification, few-shot learning, and open-set recognition in one integrated algorithm, whereas existing classification approaches focus only on one aspect and  deliver poorly over the entire class spectrum. The key challenges are how to share visual knowledge between head and tail classes and how to reduce confusion between tail and open classes. We develop an integrated OLTR algorithm that maps an image to a feature space such that visual concepts can easily relate to each other based on a learned metric that respects the closed-world classification while acknowledging the novelty of the open world. Our so-called dynamic meta-embedding combines a direct image feature and an associated memory feature, with the feature norm indicating the familiarity to known classes. On three large-scale OLTR datasets we curate from object-centric ImageNet, scene-centric Places, and face-centric MS1M data, our method consistently outperforms the state-of-the-art. Our code, datasets, and models enable future OLTR research and are publicly available.
	</p>
      </div>

<div class="section demo">
	<h2>Public Video</h2>
	<br>
	<center>
	  <iframe width="810" height="480" src="https://www.youtube.com/embed/A45wrs1g8VA" frameborder="0" allowfullscreen></iframe>
	</video>
	    </center>
	    </div>

<br>
      
<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/1904.05160" target="_blank" class="imageLink"><img src="./agda/paper.jpg"></a><br>
		  <a href="https://arxiv.org/abs/1904.05160" target="_blank">Paper</a>
		</div>
	      </li>

	      <li class="grid">
	      <div class="griditem">
		<a href="../papers/agda_poster.pdf" target="_blank" class="imageLink"><img src="./agda/poster.jpg"></a><br>
		  <a href="../papers/agda_poster.pdf" target="_blank">Poster</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>
	    
<br>

<div class="section presentation">
	<h2>Presentation</h2>
	<center>
	  <ul>
            <li class="grid">
	      <div class="griditem">
		<a href="https://www.youtube.com/watch?v=BQZ5xKd5kis&t=1361" target="_blank" class="imageLink"><img src="./agda/video.png"></a><br>
		  <a href="https://www.youtube.com/watch?v=BQZ5xKd5kis&t=1361" target="_blank">Video Recording</a>
		</div>
	      </li>
	    <li class="grid">
	      <div class="griditem">
		<a href="../papers/agda_slides.pdf" target="_blank" class="imageLink"><img src="./agda/slides.jpg"></a><br>
		  <a href="../papers/agda_slides.pdf" target="_blank">Slides</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
	    </div>

<br>

<div class="section code">
	<h2>Code and Models</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/zhmiao/OpenLongTailRecognition-OLTR" target="_blank" class="imageLink"><img src="./agda/code.png"></a><br>
		  <a href="https://github.com/zhmiao/OpenLongTailRecognition-OLTR" target="_blank">Code and Models</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section data">
	<h2>Datasets</h2>
	<br>
	<center>
      	<a href="https://drive.google.com/drive/folders/1j7Nkfe6ZhzKFXePHdsseeeGI877Xu1yf?usp=sharing" target="_blank" class="imageLink"><img src="./agda/dataset.png" border="2" width="70%"></a><br>
      	<a href="https://drive.google.com/drive/folders/1j7Nkfe6ZhzKFXePHdsseeeGI877Xu1yf?usp=sharing" target="_blank">Open Long-Tailed Datasets</a>
    </center>
    </div>

<br>

<div class="section data">
	<h2>Blog</h2>
	<br>
	<center>
      	<a href="https://bair.berkeley.edu/blog/2019/05/13/oltr/" target="_blank" class="imageLink"><img src="./agda/blog.png" border="2" width="30%"></a><br>
      	<a href="https://bair.berkeley.edu/blog/2019/05/13/oltr/" target="_blank">Berkeley AI Research Blog</a>
    </center>
    </div>

<br>

<div class="section citation">
	<h2>Citation</h2>
	<div class="section bibtex">
	  <pre>@inproceedings{openlongtailrecognition,
  title={Large-Scale Long-Tailed Recognition in an Open World},
  author={Liu, Ziwei and Miao, Zhongqi and Zhan, Xiaohang and Wang, Jiayun and Gong, Boqing and Yu, Stella X.},
  booktitle={IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}</pre>
	  </div>
      </div>

</body></html>
